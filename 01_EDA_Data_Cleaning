{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFotQDtUJy1b",
        "outputId": "eac22dee-cc53-4322-dd55-1f4076e98207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Extract Sentiment\n",
        "zip_path = '/content/drive/My Drive/AAI_Capstone/archive.zip'\n",
        "extract_path = '/content/environmental_data/'\n",
        "\n",
        "# Unzip the file.\n",
        "!unzip -q -n \"{zip_path}\" -d \"{extract_path}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Paths\n",
        "TRAIN_PATH = '/content/environmental_data/disaster_response_messages_training.csv'\n",
        "CLEAN_PATH = '/content/environmental_data/disaster_response_messages_training_CLEANED.csv'\n",
        "\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(TRAIN_PATH)\n",
        "print(f\"Initial shape: {df.shape}\")\n",
        "\n",
        "# 1. Dedup\n",
        "# Simple check before and after is usually enough for logs\n",
        "initial_rows = len(df)\n",
        "df = df.drop_duplicates()\n",
        "if len(df) < initial_rows:\n",
        "    print(f\"Dropped {initial_rows - len(df)} duplicates\")\n",
        "\n",
        "# 2. Fix target labels\n",
        "print(\"Fixing 'related' labels...\")\n",
        "df['related'] = df['related'].replace(2, 1)\n",
        "\n",
        "# 3. Handle Nulls\n",
        "# Drop rows missing the actual text content\n",
        "df = df.dropna(subset=['message'])\n",
        "\n",
        "# Fill missing original text with placeholder\n",
        "df['original'] = df['original'].fillna('N/A')\n",
        "\n",
        "# Final Sanity Check\n",
        "print(\"\\n--- Final Data Check ---\")\n",
        "# We usually just inspect the types and head briefly\n",
        "print(df.info())\n",
        "print(f\"Total nulls remaining: {df.isnull().sum().sum()}\")\n",
        "print(f\"Final shape: {df.shape}\")\n",
        "\n",
        "# Save\n",
        "df.to_csv(CLEAN_PATH, index=False)\n",
        "print(f\"\\nSaved cleaned data to: {CLEAN_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef65SNl4MAfq",
        "outputId": "53fa3bc8-2a3c-4894-a215-6369d20609fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2895356736.py:9: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(TRAIN_PATH)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape: (21046, 42)\n",
            "Dropped 19 duplicates\n",
            "Fixing 'related' labels...\n",
            "\n",
            "--- Final Data Check ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 21027 entries, 0 to 21045\n",
            "Data columns (total 42 columns):\n",
            " #   Column                  Non-Null Count  Dtype \n",
            "---  ------                  --------------  ----- \n",
            " 0   id                      21027 non-null  int64 \n",
            " 1   split                   21027 non-null  object\n",
            " 2   message                 21027 non-null  object\n",
            " 3   original                21027 non-null  object\n",
            " 4   genre                   21027 non-null  object\n",
            " 5   related                 21027 non-null  int64 \n",
            " 6   PII                     21027 non-null  int64 \n",
            " 7   request                 21027 non-null  int64 \n",
            " 8   offer                   21027 non-null  int64 \n",
            " 9   aid_related             21027 non-null  int64 \n",
            " 10  medical_help            21027 non-null  int64 \n",
            " 11  medical_products        21027 non-null  int64 \n",
            " 12  search_and_rescue       21027 non-null  int64 \n",
            " 13  security                21027 non-null  int64 \n",
            " 14  military                21027 non-null  int64 \n",
            " 15  child_alone             21027 non-null  int64 \n",
            " 16  water                   21027 non-null  int64 \n",
            " 17  food                    21027 non-null  int64 \n",
            " 18  shelter                 21027 non-null  int64 \n",
            " 19  clothing                21027 non-null  int64 \n",
            " 20  money                   21027 non-null  int64 \n",
            " 21  missing_people          21027 non-null  int64 \n",
            " 22  refugees                21027 non-null  int64 \n",
            " 23  death                   21027 non-null  int64 \n",
            " 24  other_aid               21027 non-null  int64 \n",
            " 25  infrastructure_related  21027 non-null  int64 \n",
            " 26  transport               21027 non-null  int64 \n",
            " 27  buildings               21027 non-null  int64 \n",
            " 28  electricity             21027 non-null  int64 \n",
            " 29  tools                   21027 non-null  int64 \n",
            " 30  hospitals               21027 non-null  int64 \n",
            " 31  shops                   21027 non-null  int64 \n",
            " 32  aid_centers             21027 non-null  int64 \n",
            " 33  other_infrastructure    21027 non-null  int64 \n",
            " 34  weather_related         21027 non-null  int64 \n",
            " 35  floods                  21027 non-null  int64 \n",
            " 36  storm                   21027 non-null  int64 \n",
            " 37  fire                    21027 non-null  int64 \n",
            " 38  earthquake              21027 non-null  int64 \n",
            " 39  cold                    21027 non-null  int64 \n",
            " 40  other_weather           21027 non-null  int64 \n",
            " 41  direct_report           21027 non-null  int64 \n",
            "dtypes: int64(38), object(4)\n",
            "memory usage: 6.9+ MB\n",
            "None\n",
            "Total nulls remaining: 0\n",
            "Final shape: (21027, 42)\n",
            "\n",
            "Saved cleaned data to: /content/environmental_data/disaster_response_messages_training_CLEANED.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Paths\n",
        "TRAIN_PATH = '/content/environmental_data/disaster_response_messages_training.csv'\n",
        "VAL_PATH = '/content/environmental_data/disaster_response_messages_validation.csv'\n",
        "TEST_PATH = '/content/environmental_data/disaster_response_messages_test.csv'\n",
        "\n",
        "def load_and_clean(path):\n",
        "    df = pd.read_csv(path)\n",
        "    # Standard cleaning pipeline\n",
        "    df = df.drop_duplicates()\n",
        "    if 'related' in df.columns:\n",
        "        df['related'] = df['related'].replace(2, 1)\n",
        "    df = df.dropna(subset=['message'])\n",
        "    df['original'] = df['original'].fillna('N/A')\n",
        "    return df\n",
        "\n",
        "# 1. Load & Clean\n",
        "print(\"Loading datasets...\")\n",
        "df_train = load_and_clean(TRAIN_PATH)\n",
        "df_val = load_and_clean(VAL_PATH)\n",
        "df_test = load_and_clean(TEST_PATH)\n",
        "\n",
        "# 2. Split Features (X) and Targets (y)\n",
        "# Assuming labels start at column index 4\n",
        "LABEL_START_IDX = 4\n",
        "feature_cols = ['message', 'genre']\n",
        "\n",
        "X_train, y_train = df_train[feature_cols], df_train.iloc[:, LABEL_START_IDX:]\n",
        "X_val, y_val = df_val[feature_cols], df_val.iloc[:, LABEL_START_IDX:]\n",
        "X_test, y_test = df_test[feature_cols], df_test.iloc[:, LABEL_START_IDX:]\n",
        "\n",
        "print(f\"Training shape: {X_train.shape}\")\n",
        "print(f\"Label shape: {y_train.shape}\")\n",
        "\n",
        "# 3. Build Preprocessor\n",
        "# TF-IDF on text, OneHot on metadata\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('tfidf', TfidfVectorizer(stop_words='english'), 'message'),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'), ['genre'])\n",
        "    ],\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# 4. Fit & Transform\n",
        "print(\"Fitting preprocessor...\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Sanity Check\n",
        "print(f\"Processed Feature Matrix: {X_train_processed.shape}\")\n",
        "print(f\"Matrix Type: {type(X_train_processed)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFWLSzDgR7vZ",
        "outputId": "14829ce9-8f49-4f9c-dfc1-9c46031d3d4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1284063461.py:12: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shape: (21027, 2)\n",
            "Label shape: (21027, 38)\n",
            "Fitting preprocessor...\n",
            "Processed Feature Matrix: (21027, 30945)\n",
            "Matrix Type: <class 'scipy.sparse._csr.csr_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA\n",
        "\n",
        "# checking distributions\n",
        "print(\"\\n--- Stats ---\")\n",
        "print(\"Genre split:\")\n",
        "print(df_train['genre'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nLabel frequency (edges):\")\n",
        "label_cols = df_train.iloc[:, 4:].select_dtypes(include='number')\n",
        "label_counts = label_cols.sum().sort_values()\n",
        "print(f\"Rarest: \\n{label_counts.head(3)}\")\n",
        "print(f\"Most Common: \\n{label_counts.tail(3)}\")\n",
        "\n",
        "# Check for unclassified rows\n",
        "empty_rows = (label_cols.sum(axis=1) == 0).sum()\n",
        "print(f\"\\nRows with no labels: {empty_rows} ({empty_rows/len(df_train):.1%})\")\n",
        "\n",
        "avg_len = df_train['message'].str.split().str.len().mean()\n",
        "print(f\"\\nAverage message length: {avg_len:.1f} words\")\n",
        "\n",
        "# Quick check of top words (check for stopwords/noise)\n",
        "print(\"\\nTop 10 words (raw):\")\n",
        "print(pd.Series(' '.join(df_train['message']).lower().split()).value_counts().head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WLqmuDZOoO6",
        "outputId": "e939527a-7076-4042-f7f7-8653cbc766be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Stats ---\n",
            "Genre split:\n",
            "genre\n",
            "news      0.496790\n",
            "direct    0.411471\n",
            "social    0.091739\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Label frequency (edges):\n",
            "Rarest: \n",
            "PII            0\n",
            "offer          0\n",
            "child_alone    0\n",
            "dtype: int64\n",
            "Most Common: \n",
            "weather_related     5862\n",
            "aid_related         8677\n",
            "related            15946\n",
            "dtype: int64\n",
            "\n",
            "Rows with no labels: 5081 (24.2%)\n",
            "\n",
            "Average message length: 23.7 words\n",
            "\n",
            "Top 10 words (raw):\n",
            "the    25993\n",
            "and    14688\n",
            "to     14055\n",
            "of     13470\n",
            "in     12798\n",
            "a       7798\n",
            "i       5621\n",
            "for     5257\n",
            "is      4442\n",
            "are     3852\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}
